{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Machine Learning Classifieds Model Tour on scikit-learn\n",
    "- Introduction to powerful and popular classification algorithms, logistic regression, support vector machines, and decision trees\n",
    "- Using the scikit-learn machine learning library for examples and explanations\n",
    "- Describe the strengths and weaknesses of classification algorithms with linear or nonlinear decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Select classification algorithm\n",
    "\n",
    "The predictive and computational performance of the classification model depends heavily on the data you want to use for training  \n",
    "The five main steps for training machine learning algorithms are as follows\n",
    "1. Select property and collect training samples\n",
    "2. choose performance index\n",
    "3. Choose a classification model and an optimization algorithm\n",
    "4. Evaluate model performance\n",
    "5. Tune the algorithm\n",
    "\n",
    "## 3.2 Scikit-learn First Steps: Perceptron Training\n",
    "\n",
    "Use only two properties from the iris dataset for visualization  \n",
    "In 150 flower samples, the petal length and petal width are assigned to the characteristic matrix x, and the class labels corresponding to the corresponding flower varieties are assigned to the vector y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Label: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,[2, 3]]\n",
    "y = iris.target\n",
    "print('Class Label:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The np.unique (y) function returns three unique class labels stored in iris.target  \n",
    "As you can see, Iris-setosa, Iris-versicolor, and Iris-virginica are already stored as integers (here: 0,1,2)  \n",
    "Integer labels are recommended because they avoid small numbers and take up small memory areas, which improves computational performance  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly split X and y arrays using the train_test_split function of the scikit-learn model_selection module  \n",
    "30% will be test data, 70% will be training data  \n",
    "\n",
    "\n",
    "premix the dataset before the train_test_split function splits  \n",
    "Otherwise, class 0 and class 1 are in the training set, and the test set consists of only 45 samples of class 2  \n",
    "Passes a fixed ramdon seed (random_state = 1) to the random_state parameter to the pseudorandom number generator used to randomly shuffle the dataset before splitting  \n",
    "Fixing random_state can reproduce the result of execution  \n",
    "\n",
    "Finally use stratification via stratify = y  \n",
    "Stratification means that the train_test_split function makes the ratio of class labels in the training set and test set equal to the input data set  \n",
    "You can count the number of unique values ​​in an array using the numpy bincount function  \n",
    "\n",
    "\n",
    "Let's check the stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label count for y [50 50 50]\n"
     ]
    }
   ],
   "source": [
    "print('label count for y', np.bincount(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label count for y_train [35 35 35]\n"
     ]
    }
   ],
   "source": [
    "print('label count for y_train', np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label count for y_test [15 15 15]\n"
     ]
    }
   ],
   "source": [
    "print('label count for y_test', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will standardize the characteristics using the StandardScaler class from scikit-learn's preprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit method of the StandardScaler calculates $\\mu$ and $\\sigma$ for each feature dimension in the training set  \n",
    "Calling the transform method normalizes the training set using the computed $\\mu$ and $\\sigma$  \n",
    "Standardize the test set using the same $\\mu$ and $\\sigma$ so that samples from the training and test sets are moved at the same rate  \n",
    "\n",
    "Standardize training data and train perceptron models  \n",
    "Most of scikit-learn's algorithms support multiclass classifications using the OVR method  \n",
    "I will inject three iris classes into the perceptron at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=0.1,\n",
       "           fit_intercept=True, max_iter=40, n_iter_no_change=5, n_jobs=None,\n",
       "           penalty=None, random_state=1, shuffle=True, tol=0.001,\n",
       "           validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "ppn = Perceptron(max_iter=40, eta0=0.1, tol=1e-3, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Perceptron class from the linear_model module, create a new Perceptron object, and train the model using the fit method  \n",
    "\n",
    "Some experimentation is required to find an appropriate learning rate  \n",
    "If the learning rate is too high, the algorithm goes past the global minimum  \n",
    "If the learning rate is too small, the learning rate is slow, which requires a lot of epochs to converge, especially on large datasets  \n",
    "Use the random_state parameter so that the results of mixing the training set per epoch are reproduced later  \n",
    "\n",
    "You can make predictions with the predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified Sample Count: 1\n"
     ]
    }
   ],
   "source": [
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified Sample Count: %d' % (y_test != y_pred).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
